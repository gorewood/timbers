---
title: "The LLM Bridge"
date: 2026-01-19T19:40:00Z
---

I spent the weekend making this tool look like it actually belongs in 2026.

The CLI output was functional but grim—just raw text dumping to stdout like some shell script I'd written in college. It worked, but every time I ran `timbers list` or looked at an entry, I winced a little. **So I pulled in lipgloss and built a little styling layer on top of it.**

The approach was pretty straightforward: a `Printer` with helpers for common patterns. `Table()` for tabular data, `Box()` for callouts, `Section()` for headers, `KeyValue()` for pairs. Nothing fancy, but it meant I could **stop manually padding strings and guessing at terminal widths**. The printer detects TTY automatically, so piped output stays clean while interactive usage gets the full treatment.

Then I went through every command and swapped out the bare `fmt.Printf` calls. The diff touched a bunch of files, but each change was tiny—just wrapping existing output in the new helpers. `timbers show` now renders entries in actual boxes. Lists look like tables instead of accidentally-formatted chaos. It's the kind of polish that doesn't add features but makes the tool *feel* like someone cares about it.

The other chunk of work was **LLM plumbing**. I added a multi-provider client that can talk to OpenAI, Anthropic, or anything else that speaks a compatible API. Two new commands: `generate` to create a devlog entry from a commit range, and `catchup` to backfill missing entries in one pass.

The interesting part wasn't the API calls—it was making the whole thing testable and bulletproof. I introduced an `HTTPDoer` interface so I could mock the HTTP layer in tests without spinning up fake servers. **Error handling got a serious upgrade**: truncated response bodies before logging them (nobody needs 50KB of HTML in their error message), validated inputs before hitting the network, converted internal errors to proper `ExitError` types so the CLI doesn't leak stack traces to users.

There were a couple of surprises. Empty LLM responses needed explicit handling—turns out some providers return 200 with no content under certain conditions. And the input validation saved me from a few embarrassing bugs where I'd have sent malformed requests and gotten cryptic 400s back.

The tool now *looks* like a tool and can actually generate its own devlogs. Which is either recursive elegance or peak navel-gazing, depending on your mood.

---
*Written with AI assistance as a formatting aid.*
